{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZ7mRM9YA52L"
   },
   "source": [
    "# RNN : 자연어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_axSJoRIA52N"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysPzvAuCA52V"
   },
   "source": [
    "## 단어 단위 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "WxsCRB7cA52Z",
    "outputId": "f2910ab7-7503-43b7-cfac-091127755414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://bit.ly/2Mc3SOV\n",
      "62013440/62012502 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 7.32 조선왕조실록 데이터 파일 다운로드\n",
    "# pd.read_csv 와 같음 --> get_file\n",
    "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "KT6VZBi0A52f",
    "outputId": "ec4dd435-b390-43c9-e9c5-7ce48219e44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 26265493 characters\n",
      "\n",
      "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
      "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n"
     ]
    }
   ],
   "source": [
    "# 7.33 데이터 로드 및 확인\n",
    "# 데이터를 메모리에 불러옵니다. encoding 형식으로 utf-8 을 지정해야합니다.\n",
    "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 텍스트가 총 몇 자인지 확인합니다.\n",
    "print('Length of text: {} characters'.format(len(train_text)))\n",
    "print()\n",
    "\n",
    "# 처음 100 자를 확인해봅니다.\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rX-BRHkoA52k"
   },
   "source": [
    "### 한글 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "y6mpJP0uA52m",
    "outputId": "6967192f-467b-4738-b579-f223324fb917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n"
     ]
    }
   ],
   "source": [
    "# 7.34 훈련 데이터 입력 정제\n",
    "import re\n",
    "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "def clean_str(string):    \n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "train_text = train_text.split('\\n')\n",
    "train_text = [clean_str(sentence) for sentence in train_text]\n",
    "train_text_X = []\n",
    "for sentence in train_text:\n",
    "    train_text_X.extend(sentence.split(' '))\n",
    "    train_text_X.append('\\n')\n",
    "    \n",
    "train_text_X = [word for word in train_text_X if word != '']\n",
    "\n",
    "print(train_text_X[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EripQTWuA52s"
   },
   "source": [
    "### 단어 - 키값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "9utmt9k9A52t",
    "outputId": "a87a6f64-a549-4887-9171-1789bfc2f4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332640 unique words\n",
      "{\n",
      "  '\\n':   0,\n",
      "  '!' :   1,\n",
      "  ',' :   2,\n",
      "  '000명으로':   3,\n",
      "  '001':   4,\n",
      "  '002':   5,\n",
      "  '003':   6,\n",
      "  '004':   7,\n",
      "  '005':   8,\n",
      "  '006':   9,\n",
      "  ...\n",
      "}\n",
      "index of UNK: 332639\n"
     ]
    }
   ],
   "source": [
    "# 7.35 단어 토큰화\n",
    "# 단어의 set을 만듭니다.\n",
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print ('{} unique words'.format(len(vocab)))\n",
    "\n",
    "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
    "\n",
    "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
    "print('{')\n",
    "for word,_ in zip(word2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "print('index of UNK: {}'.format(word2idx['UNK']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J74AR1wLA52y"
   },
   "source": [
    "### 실제 문장을 단어키값들의 문장들로 재구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "id": "23ecy_OEA52z",
    "outputId": "ecbf42e4-e981-4e00-a37c-77479a80bd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조', '이성계', '선대의', '가계', '목조', '이안사가', '전주에서', '삼척', '의주를', '거쳐', '알동에', '정착하다', '\\n', '태조', '강헌', '지인', '계운', '성문', '신무', '대왕']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413]\n"
     ]
    }
   ],
   "source": [
    "# 7.36 토큰 데이터 확인\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3_QIvFUA525"
   },
   "source": [
    "### 기본 데이터셋 구성하기 --> train_test_split이 아니라 tf.data.dataset을 이용해서..\n",
    "- Dataset의 장점 : 간단한 코드로 데이터 섞기, 배치수 만큰 자르기, 다른 Dataset에 매핑하기 등 빠르게 수행이 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "_YmTNJlwA526",
    "outputId": "cfd5f882-a8ab-40e3-c351-f5ce2bf33e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',' '휘']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2 330313]\n"
     ]
    }
   ],
   "source": [
    "# 7.37 기본 데이터셋 만들기\n",
    "\n",
    "# seq_length ====> 25개의 단어가 주어졌을 때, 다음 단어를 예측하도록 데이터를 만들려고 함.\n",
    "seq_length = 25\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "\n",
    "# 데이터 생성하기\n",
    "# ---> batch :  한 번에 반환하는 데이터의 숫자 지정\n",
    "#               25+1을 한 이유는 입력으로 할 25개와 그 다음에 1개를 주고 이제 시작하기 위해서..25+1\n",
    "# ---> drop_remainder :  남는거 버리기...\n",
    "# ------> (입력 25단어, 1단어)\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "\n",
    "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx2word[item.numpy()])\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "colab_type": "code",
    "id": "Oz5isXbdA529",
    "outputId": "fa03cd21-b019-453f-88fb-466f4a97f9f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['태조' '이성계' '선대의' '가계' '목조' '이안사가' '전주에서' '삼척' '의주를' '거쳐' '알동에' '정착하다'\n",
      " '\\n' '태조' '강헌' '지인' '계운' '성문' '신무' '대왕' '의' '성은' '이씨' '요' ',']\n",
      "[299305 229634 161443  17430 111029 230292 251081 155087 225462  29027\n",
      " 190295 256129      0 299305  25624 273553  36147 163996 180466  84413\n",
      " 224182 164549 230248 210912      2]\n",
      "휘\n",
      "330313\n"
     ]
    }
   ],
   "source": [
    "# 7.38 학습 데이터셋 만들기\n",
    "# 26개의 단어를 25개의 단어와 1개의 시작용 단어로 분리하는 부분\n",
    "# 그리고 tf의 map을 이용해서 데이터셋을 만들기..\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "# pandas의 map이 아니다. tensorflow의 map\n",
    "train_dataset = sentence_dataset.map(split_input_target)\n",
    "for x,y in train_dataset.take(1):\n",
    "    print(idx2word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2word[y.numpy()])\n",
    "    print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1ZF1c_uA53B"
   },
   "source": [
    "### 데이터셋 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxdoo18sA53C"
   },
   "outputs": [],
   "source": [
    "# 7.39 데이터셋 shuffle, batch 설정\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "\n",
    "# tf.data는 대용량 서버에 대응할 수 있도록 구현이 되어 있어서\n",
    "# 일정량의 데이터를 버퍼에 두고 할 수 있도록 함.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIygpTZBA53G"
   },
   "source": [
    "### 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "rKN7ajibA53G",
    "outputId": "2fa90198-2719-49be-bbd8-551c8c089b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 100)           33264000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 25, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 332640)            33596640  \n",
      "=================================================================\n",
      "Total params: 67,021,440\n",
      "Trainable params: 67,021,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.40 단어 단위 생성 모델 정의\n",
    "total_words = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units=100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzuFlX5UA53L"
   },
   "source": [
    "### 모델 학습 시키기 --> 엄청 오래걸림;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "2fuC31I9A53M",
    "outputId": "68b50888-31a8-40c0-9e49-60faeb2308c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 494/2135 [=====>........................] - ETA: 6:10 - loss: 9.6460 - accuracy: 0.0684"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1123bdaa738c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m##### 너무 오래 걸려서 epoch 2로 수정함;;;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtestmodelcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 7.41 단어 단위 생성 모델 학습\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    test_sentence = train_text[0]\n",
    "\n",
    "    next_words = 100\n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "\n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "    \n",
    "    print()\n",
    "    print(test_sentence)\n",
    "    print()\n",
    "\n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "\n",
    "##### 너무 오래 걸려서 epoch 2로 수정함;;;\n",
    "history = model.fit(train_dataset.repeat(), epochs=2, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlCJy8CPA53S"
   },
   "source": [
    "### 임의의 문장을 사용해서 생성 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PeFQtgdA53S"
   },
   "outputs": [],
   "source": [
    "# 7.42 임의의 문장을 사용한 생성 결과 확인\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
    "\n",
    "next_words = 100\n",
    "for _ in range(next_words):\n",
    "    test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "    test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "    \n",
    "    output_idx = model.predict_classes(test_text_X)\n",
    "    test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqlCewcRA53V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhcGIh04A53a"
   },
   "source": [
    "## 자소 단위로 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3477r5h7A53b"
   },
   "source": [
    "### 기본적인 설치 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "colab_type": "code",
    "id": "NnEu-zT7A53c",
    "outputId": "06ae0b91-9f01-47bf-f848-c8c7e2f11c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jamotools\n",
      "  Downloading https://files.pythonhosted.org/packages/3d/d6/ec13c68f7ea6a8085966390d256d183bf8488f8b9770028359acb86df643/jamotools-0.1.10-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.18.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jamotools) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from jamotools) (0.16.0)\n",
      "Installing collected packages: jamotools\n",
      "Successfully installed jamotools-0.1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install jamotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "id": "UepQvsvTA53f",
    "outputId": "80e29698-9c57-41ac-bbc9-334399555dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
      "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n",
      "﻿ㅌㅐㅈㅗ ㅇㅣㅅㅓㅇㄱㅖ ㅅㅓㄴㄷㅐㅇㅢ ㄱㅏㄱㅖ. ㅁㅗㄱㅈㅗ ㅇㅣㅇㅏㄴㅅㅏㄱㅏ ㅈㅓㄴㅈㅜㅇㅔㅅㅓ ㅅㅏㅁㅊㅓㄱ·ㅇㅢㅈㅜㄹㅡㄹ ㄱㅓㅊㅕ ㅇㅏㄹㄷㅗㅇㅇㅔ ㅈㅓㅇㅊㅏㄱㅎㅏㄷㅏ \n",
      "ㅌㅐㅈㅗ ㄱㅏㅇㅎㅓㄴ ㅈㅣㅇㅣㄴ ㄱㅖㅇㅜㄴ ㅅㅓㅇㅁㅜㄴ ㅅㅣㄴㅁㅜ ㄷㅐㅇㅘㅇ(太祖康獻至仁啓運聖文神武大王)ㅇㅢ ㅅㅓㅇㅇㅡㄴ ㅇㅣㅆㅣ(李氏)ㅇㅛ, ㅎㅟ\n"
     ]
    }
   ],
   "source": [
    "# 7.44 자모 분리 테스트\n",
    "import jamotools\n",
    "\n",
    "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "s = train_text[:100]\n",
    "print(s)\n",
    "\n",
    "# 한글 텍스트를 자모 단위로 분리해줍니다. 한자 등에는 영향이 없습니다.\n",
    "s_split = jamotools.split_syllables(s)\n",
    "print(s_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bICECekaA53j"
   },
   "source": [
    "### 잠시 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "bm478sIEA53j",
    "outputId": "4551bfd6-f84a-4d0a-cdb8-a2a69b55aa5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿태조 이성계 선대의 가계. 목조 이안사가 전주에서 삼척·의주를 거쳐 알동에 정착하다 \n",
      "태조 강헌 지인 계운 성문 신무 대왕(太祖康獻至仁啓運聖文神武大王)의 성은 이씨(李氏)요, 휘\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 7.45 자모 결합 테스트 ---> 자모를 분리를 하였다가 합쳐도 동일함.\n",
    "s2 = jamotools.join_jamos(s_split)\n",
    "print(s2)\n",
    "print(s == s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eX5FfSunA53o"
   },
   "source": [
    "### 자모를 대상으로 토큰화 진행\n",
    "- 여기서는 따로 전처리 생략 --> 괄호, 이상한거 다 들어감;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "vw3qBgEBA53o",
    "outputId": "39ea01e3-4f85-4f0e-c60b-08262f1b8524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6198 unique characters\n",
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  \"'\" :   4,\n",
      "  '(' :   5,\n",
      "  ')' :   6,\n",
      "  '+' :   7,\n",
      "  ',' :   8,\n",
      "  '-' :   9,\n",
      "  ...\n",
      "}\n",
      "index of UNK: 6197\n"
     ]
    }
   ],
   "source": [
    "# 7.46 자모 토큰화\n",
    "# 텍스트를 자모 단위로 나눕니다. 데이터가 크기 때문에 약간 시간이 걸립니다.\n",
    "train_text_X = jamotools.split_syllables(train_text)\n",
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in train_text_X])\n",
    "\n",
    "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "\n",
    "print('index of UNK: {}'.format(char2idx['UNK']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iACDQ72A53t"
   },
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "LkDvvdK4A53t",
    "outputId": "fdf060de-fa36-4114-8caf-106668071c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ㅌㅐㅈㅗ ㅇㅣㅅㅓㅇㄱㅖ ㅅㅓㄴㄷㅐㅇ\n",
      "[6158   83   87   79   94    1   78  106   76   90   78   56   93    1\n",
      "   76   90   59   62   87   78]\n"
     ]
    }
   ],
   "source": [
    "# 7.47 토큰 데이터 확인\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4WgCBmNA53x"
   },
   "source": [
    "### 데이터셋 생성\n",
    "- 단, 여기서는 자모음으로 입력을 받는 것이기 때문에 25정도의 단어에 해당하게 80자의 자모음을 입력으로 받아서 하도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "NMC3tl7fA53x",
    "outputId": "7f9fb166-015f-4884-acb2-d2ebc5834029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff' 'ㅌ' 'ㅐ' 'ㅈ' 'ㅗ' ' ' 'ㅇ' 'ㅣ' 'ㅅ' 'ㅓ' 'ㅇ' 'ㄱ' 'ㅖ' ' ' 'ㅅ' 'ㅓ' 'ㄴ'\n",
      " 'ㄷ' 'ㅐ' 'ㅇ' 'ㅢ' ' ' 'ㄱ' 'ㅏ' 'ㄱ' 'ㅖ' '.' ' ' 'ㅁ' 'ㅗ' 'ㄱ' 'ㅈ' 'ㅗ' ' ' 'ㅇ'\n",
      " 'ㅣ' 'ㅇ' 'ㅏ' 'ㄴ' 'ㅅ' 'ㅏ' 'ㄱ' 'ㅏ' ' ' 'ㅈ' 'ㅓ' 'ㄴ' 'ㅈ' 'ㅜ' 'ㅇ' 'ㅔ' 'ㅅ' 'ㅓ'\n",
      " ' ' 'ㅅ' 'ㅏ' 'ㅁ' 'ㅊ' 'ㅓ' 'ㄱ' '·' 'ㅇ' 'ㅢ' 'ㅈ' 'ㅜ' 'ㄹ' 'ㅡ' 'ㄹ' ' ' 'ㄱ' 'ㅓ'\n",
      " 'ㅊ' 'ㅕ' ' ' 'ㅇ' 'ㅏ' 'ㄹ' 'ㄷ' 'ㅗ' 'ㅇ' 'ㅇ']\n",
      "[6158   83   87   79   94    1   78  106   76   90   78   56   93    1\n",
      "   76   90   59   62   87   78  105    1   56   86   56   93   10    1\n",
      "   72   94   56   79   94    1   78  106   78   86   59   76   86   56\n",
      "   86    1   79   90   59   79   99   78   91   76   90    1   76   86\n",
      "   72   81   90   56   36   78  105   79   99   64  104   64    1   56\n",
      "   90   81   92    1   78   86   64   62   94   78   78]\n",
      "['\\ufeff' 'ㅌ' 'ㅐ' 'ㅈ' 'ㅗ' ' ' 'ㅇ' 'ㅣ' 'ㅅ' 'ㅓ' 'ㅇ' 'ㄱ' 'ㅖ' ' ' 'ㅅ' 'ㅓ' 'ㄴ'\n",
      " 'ㄷ' 'ㅐ' 'ㅇ' 'ㅢ' ' ' 'ㄱ' 'ㅏ' 'ㄱ' 'ㅖ' '.' ' ' 'ㅁ' 'ㅗ' 'ㄱ' 'ㅈ' 'ㅗ' ' ' 'ㅇ'\n",
      " 'ㅣ' 'ㅇ' 'ㅏ' 'ㄴ' 'ㅅ' 'ㅏ' 'ㄱ' 'ㅏ' ' ' 'ㅈ' 'ㅓ' 'ㄴ' 'ㅈ' 'ㅜ' 'ㅇ' 'ㅔ' 'ㅅ' 'ㅓ'\n",
      " ' ' 'ㅅ' 'ㅏ' 'ㅁ' 'ㅊ' 'ㅓ' 'ㄱ' '·' 'ㅇ' 'ㅢ' 'ㅈ' 'ㅜ' 'ㄹ' 'ㅡ' 'ㄹ' ' ' 'ㄱ' 'ㅓ'\n",
      " 'ㅊ' 'ㅕ' ' ' 'ㅇ' 'ㅏ' 'ㄹ' 'ㄷ' 'ㅗ' 'ㅇ']\n",
      "[6158   83   87   79   94    1   78  106   76   90   78   56   93    1\n",
      "   76   90   59   62   87   78  105    1   56   86   56   93   10    1\n",
      "   72   94   56   79   94    1   78  106   78   86   59   76   86   56\n",
      "   86    1   79   90   59   79   99   78   91   76   90    1   76   86\n",
      "   72   81   90   56   36   78  105   79   99   64  104   64    1   56\n",
      "   90   81   92    1   78   86   64   62   94   78]\n",
      "ㅇ\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "# 7.48 학습 데이터세트 생성\n",
    "seq_length = 80  # --> 단어로 25글자를 입력으로 출력으로 다음에 올 1개의 단어를 했던 모들을\n",
    "# --> 단어가 아니라 자음/모음/에 대한 것들로 입력을 하게 하니\n",
    "# --> 단어로 25 단어들의 대략 해당하는 자모음의 배열이 80개로 매칭을 함 >> 그 다음에 나올 자음, 모음\n",
    "\n",
    "# 맨 처음에 대한 것은 어절 25개 입력 >> 출력 1개의 어절\n",
    "# 지금은 자음/모음 순서 80개를 입력 >> 출력 1개의 자음 or 모음\n",
    "\n",
    "examples_per_epoch = len(text_as_int) // seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "char_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in char_dataset.take(1):\n",
    "    print(idx2char[item.numpy()])\n",
    "    print(item.numpy())\n",
    "    \n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = char_dataset.map(split_input_target)\n",
    "for x,y in train_dataset.take(1):\n",
    "    print(idx2char[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2char[y.numpy()])\n",
    "    print(y.numpy())\n",
    "    \n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eN02jY__A530"
   },
   "source": [
    "### 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "8EaJDM1FA530",
    "outputId": "63e06251-b0d8-41d4-dab5-2bf8dcb9e56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 100)           619800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6198)              2485398   \n",
      "=================================================================\n",
      "Total params: 3,906,798\n",
      "Trainable params: 3,906,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7.49 자소 단위 생성 모델 정의\n",
    "total_chars = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=400),\n",
    "    tf.keras.layers.Dense(total_chars, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVBUQtaXA534"
   },
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROa1OquoA534"
   },
   "outputs": [],
   "source": [
    "# 7.50 자소 단위 생성 모델 학습\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 99:\n",
    "        return\n",
    "    \n",
    "    test_sentence = train_text[:48]\n",
    "    test_sentence = jamotools.split_syllables(test_sentence)\n",
    "\n",
    "    next_chars = 300\n",
    "    for _ in range(next_chars):\n",
    "        test_text_X = test_sentence[-seq_length:]\n",
    "        test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
    "\n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += idx2char[output_idx[0]]\n",
    "    \n",
    "    print()\n",
    "    print(jamotools.join_jamos(test_sentence))\n",
    "    print()\n",
    "\n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "########---> epoch=2로 줄여둠;;;\n",
    "history = model.fit(train_dataset.repeat(), epochs=2, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gtdP-zGA539"
   },
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyzI-raxA539"
   },
   "outputs": [],
   "source": [
    "# 7.51 임의의 문장을 사용한 생성 결과 확인\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
    "test_sentence = jamotools.split_syllables(test_sentence)\n",
    "\n",
    "next_chars = 300\n",
    "for _ in range(next_chars):\n",
    "    test_text_X = test_sentence[-seq_length:]\n",
    "    test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
    "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
    "    \n",
    "    output_idx = model.predict_classes(test_text_X)\n",
    "    test_sentence += idx2char[output_idx[0]]\n",
    "    \n",
    "\n",
    "print(jamotools.join_jamos(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTQxCkFvA54B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cOwD6gqA54E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4edn8HgA54H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "20200818_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
